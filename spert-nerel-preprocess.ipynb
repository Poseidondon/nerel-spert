{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8187474,"sourceType":"datasetVersion","datasetId":4832727},{"sourceId":8188767,"sourceType":"datasetVersion","datasetId":4848442}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Setup","metadata":{}},{"cell_type":"code","source":"import json\nimport random\nimport nltk\nnltk.download('punkt')\n\nfrom tqdm.auto import tqdm\nfrom typing import List\n\nfrom nltk.tokenize import sent_tokenize, word_tokenize","metadata":{"execution":{"iopub.status.busy":"2024-04-21T23:19:48.455597Z","iopub.execute_input":"2024-04-21T23:19:48.456118Z","iopub.status.idle":"2024-04-21T23:19:48.466136Z","shell.execute_reply.started":"2024-04-21T23:19:48.456069Z","shell.execute_reply":"2024-04-21T23:19:48.464621Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Config","metadata":{}},{"cell_type":"code","source":"class Config:\n    val_size = 0.15\n    seed = 42\n    \n    do_split = False\n    save_nerel = True\n\nrandom.seed(Config.seed)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T23:19:48.468788Z","iopub.execute_input":"2024-04-21T23:19:48.469148Z","iopub.status.idle":"2024-04-21T23:19:48.480100Z","shell.execute_reply.started":"2024-04-21T23:19:48.469118Z","shell.execute_reply":"2024-04-21T23:19:48.478728Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"markdown","source":"### Read data","metadata":{}},{"cell_type":"code","source":"train_sentences = []\n\nwith open(\"/kaggle/input/ner-datasets/datasets/nerel/nerel_train_raw.jsonl\") as f:\n    for line in f:\n        train_sentences.append(json.loads(line))\n        #train_sentences[-1]['sentences'] = train_sentences[-1]['sentences'].split('\\n\\n')\n\ndev_sentences = []\n\nwith open(\"/kaggle/input/ner-datasets/datasets/nerel/nerel_dev_raw.jsonl\") as f:\n    for line in f:\n        line = json.loads(line)\n        line['sentences'] = line['senences']\n        del line['senences']\n        dev_sentences.append(line)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T23:19:48.481880Z","iopub.execute_input":"2024-04-21T23:19:48.482227Z","iopub.status.idle":"2024-04-21T23:19:48.574892Z","shell.execute_reply.started":"2024-04-21T23:19:48.482199Z","shell.execute_reply":"2024-04-21T23:19:48.573537Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"markdown","source":"### Split train","metadata":{}},{"cell_type":"code","source":"if Config.do_split:\n    random.shuffle(train_sentences)\n    split_ix = int(len(train_sentences) * Config.val_size)\n    train_train_sentences = train_sentences[split_ix:]\n    train_val_sentences = train_sentences[:split_ix]\n    \n    with open('nerel_train_train_raw.jsonl', 'w', encoding='utf-8') as f:\n        for line in train_train_sentences:\n            print(json.dumps(line), file=f)\n    \n    with open('nerel_train_val_raw.jsonl', 'w', encoding='utf-8') as f:\n        for line in train_val_sentences:\n            print(json.dumps(line), file=f)\nelse:\n    train_train_sentences = []\n    with open(\"/kaggle/input/ner-datasets/datasets/nerel/nerel_train_train_raw.jsonl\") as f:\n        for line in f:\n            train_train_sentences.append(json.loads(line))\n            \n    train_val_sentences = []\n    with open(\"/kaggle/input/ner-datasets/datasets/nerel/nerel_train_val_raw.jsonl\") as f:\n        for line in f:\n            train_val_sentences.append(json.loads(line))","metadata":{"execution":{"iopub.status.busy":"2024-04-21T23:19:48.576686Z","iopub.execute_input":"2024-04-21T23:19:48.577023Z","iopub.status.idle":"2024-04-21T23:19:48.950729Z","shell.execute_reply.started":"2024-04-21T23:19:48.576995Z","shell.execute_reply":"2024-04-21T23:19:48.949681Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"print(len(train_train_sentences), len(train_val_sentences))","metadata":{"execution":{"iopub.status.busy":"2024-04-21T23:19:48.953218Z","iopub.execute_input":"2024-04-21T23:19:48.954440Z","iopub.status.idle":"2024-04-21T23:19:48.961347Z","shell.execute_reply.started":"2024-04-21T23:19:48.954403Z","shell.execute_reply":"2024-04-21T23:19:48.959837Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stdout","text":"442 77\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Preprocess functions","metadata":{}},{"cell_type":"code","source":"def tokens_to_indices(text: str, tokens: list[str]):\n    token_ix = 0\n    token_char_ix = 0\n    for i, char in enumerate(text):\n        # check if token was found\n        if len(tokens[token_ix]) == token_char_ix:\n            yield i - token_char_ix, i - 1\n            token_char_ix = 0\n            token_ix += 1\n            \n            # check if all tokens returned\n            if len(tokens) == token_ix:\n                break\n        \n        # fix quoting bug\n        if (tokens[token_ix] == \"``\" or tokens[token_ix] == \"''\") and char == '\\\"':\n            yield i, i\n            token_char_ix = 0\n            token_ix += 1\n            \n            # check if all tokens returned\n            if len(tokens) == token_ix:\n                break\n\n        if tokens[token_ix][token_char_ix] == char:\n            token_char_ix += 1\n        else:\n            token_char_ix = 0\n            \n    # token not found error\n    if len(tokens) != token_ix:\n        if len(tokens[token_ix]) == token_char_ix:\n            yield i + 1 - token_char_ix, i\n            token_char_ix = 0\n            token_ix += 1\n        else:\n            print(char)\n            raise ValueError(f\"Token '{tokens[token_ix]}' by index {token_ix} not found\")\n\ndef index_to_tokens(token_indices: list[tuple[int]], index: tuple[int]):\n    token_overlaps = None\n    for i, token_ix in enumerate(token_indices):\n        # before overlap\n        if token_ix[1] < index[0]:\n            continue\n        # overlap\n        if token_ix[0] <= index[1]:\n            if token_overlaps is None:\n                token_overlaps = (i, i + 1)\n            else:\n                token_overlaps = (token_overlaps[0], i + 1)\n        # no overlap\n        if token_ix[0] > index[1]:\n            return token_overlaps\n    return token_overlaps","metadata":{"execution":{"iopub.status.busy":"2024-04-21T23:19:48.963104Z","iopub.execute_input":"2024-04-21T23:19:48.963483Z","iopub.status.idle":"2024-04-21T23:19:48.979466Z","shell.execute_reply.started":"2024-04-21T23:19:48.963451Z","shell.execute_reply":"2024-04-21T23:19:48.978171Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"markdown","source":"### Transformation functions","metadata":{}},{"cell_type":"code","source":"def nerel_to_sentinfo(nerel_data: List[dict], train=True) -> List[dict]:\n    sentences = []\n    for batch_ix, nerel_batch in tqdm(enumerate(nerel_data), total=len(nerel_data), desc=\"Parsing NEREL data\"):\n        sentence_tokens = sent_tokenize(nerel_batch['sentences'], language='russian')\n        for sent_ix, sent_offset in enumerate(tokens_to_indices(nerel_batch['sentences'], sentence_tokens)):\n            sent = sentence_tokens[sent_ix]\n            \n            # token info\n            word_tokens = word_tokenize(sent, language='russian')\n            tokens = []\n            tokens_offsets = []\n            for token, token_offset in zip(word_tokens, tokens_to_indices(sent, word_tokens)):\n                tokens.append(token)\n                tokens_offsets.append(token_offset)\n            \n            # ner info\n            if train:\n                ners = []\n                for ner in nerel_batch['ners']:\n                    # compute offset relative to sentence\n                    ner_offset = ner[0] - sent_offset[0], ner[1] - sent_offset[0]\n                    sent_len = sent_offset[1] - sent_offset[0]\n\n                    # skip ners in other sentences\n                    if ner_offset[1] < 0 or ner_offset[0] > sent_len:\n                        continue\n\n                    ner_indices = index_to_tokens(tokens_offsets, (ner_offset[0], ner_offset[1]))\n                    if ner_indices:\n                        ners.append({\"type\": ner[2], \"start\": ner_indices[0], \"end\": ner_indices[1]})\n                    else:\n                        print(tokens_offsets, (ner_offset[0], ner_offset[1]))\n                        assert False, 'Token not found'\n            \n                sentences.append({\n                    'sentence': sentence_tokens[sent_ix],\n                    'sent_offset': sent_offset,\n                    'tokens': tokens,\n                    'tokens_offsets': tokens_offsets,\n                    'batch_ix': batch_ix,\n                    'sent_ix': sent_ix,\n                    'ners': ners\n                })\n            else:\n                sentences.append({\n                    'sentence': sentence_tokens[sent_ix],\n                    'sent_offset': sent_offset,\n                    'tokens': tokens,\n                    'tokens_offsets': tokens_offsets,\n                    'batch_ix': batch_ix,\n                    'sent_ix': sent_ix,\n                    'id': nerel_batch['id']\n                })\n    \n    return sentences\n\ndef sentinfo_to_spert(sentinfo: List[dict]) -> List[dict]:\n    return [{\n        'tokens': sent['tokens'],\n        'entities': sent['ners'],\n        'relations': []\n    } for sent in sentinfo]\n\ndef sentinfo_to_spert_pred(sentinfo: List[dict]) -> List[dict]:\n    return [{'tokens': sent['tokens']} for sent in sentinfo]\n\ndef pred_to_nerel(batches_count: int, sentinfo: List[dict], pred_data: List[dict]) -> List[dict]:\n    ners = [[] for _ in range(batches_count)]\n    \n    for sent, pred in zip(sentinfo[5:], pred_data[5:]):\n        for ner in pred['entities']:\n            offsets = (\n                sent['sent_offset'][0] + sent['tokens_offsets'][ner['start']][0],\n                sent['sent_offset'][0] + sent['tokens_offsets'][ner['end'] - 1][1]\n            )\n            nerel_format = [offsets[0], offsets[1], ner['type']]\n            ners[sent['batch_ix']].append(nerel_format)\n    \n    return ners","metadata":{"execution":{"iopub.status.busy":"2024-04-21T23:19:48.981356Z","iopub.execute_input":"2024-04-21T23:19:48.981824Z","iopub.status.idle":"2024-04-21T23:19:49.005444Z","shell.execute_reply.started":"2024-04-21T23:19:48.981781Z","shell.execute_reply":"2024-04-21T23:19:49.004193Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"markdown","source":"### Convert SPERT predictions to NEREL","metadata":{}},{"cell_type":"code","source":"# load spert prediction\nwith open('/kaggle/input/nerel-predictions/predictions/dev/rubert-large-15/rubert-large-15.json', 'r') as f:\n    spert_prediction = json.load(f)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T23:19:49.007436Z","iopub.execute_input":"2024-04-21T23:19:49.007858Z","iopub.status.idle":"2024-04-21T23:19:49.064227Z","shell.execute_reply.started":"2024-04-21T23:19:49.007826Z","shell.execute_reply":"2024-04-21T23:19:49.062525Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"# load NEREL input\npred_sentences = []\n\nwith open(\"/kaggle/input/ner-datasets/datasets/nerel/nerel_dev_raw.jsonl\") as f:\n    for line in f:\n        line = json.loads(line)\n        line['sentences'] = line['senences']\n        del line['senences']\n        pred_sentences.append(line)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T23:19:49.066161Z","iopub.execute_input":"2024-04-21T23:19:49.066513Z","iopub.status.idle":"2024-04-21T23:19:49.080569Z","shell.execute_reply.started":"2024-04-21T23:19:49.066482Z","shell.execute_reply":"2024-04-21T23:19:49.079330Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"# convert NEREL input to sentinfo\npred_sentinfo = nerel_to_sentinfo(pred_sentences, train=False)\nners = pred_to_nerel(len(pred_sentences), pred_sentinfo, spert_prediction)\n\n# write to file\nwith open('test.jsonl', 'w', encoding='utf-8') as f:\n    for batch, ner_line in zip(pred_sentences, ners):\n        print(json.dumps({\"id\": batch[\"id\"], \"ners\": ner_line}), file=f)\n\n# for batch_ix, ner_line in enumerate(ners):\n#     print(pred_sentences[batch_ix]['sentences'])\n#     for ner in ner_line:\n#         print(ner, pred_sentences[batch_ix]['sentences'][ner[0]:ner[1] + 1])\n#     break","metadata":{"execution":{"iopub.status.busy":"2024-04-21T23:19:49.081956Z","iopub.execute_input":"2024-04-21T23:19:49.082376Z","iopub.status.idle":"2024-04-21T23:19:49.687339Z","shell.execute_reply.started":"2024-04-21T23:19:49.082327Z","shell.execute_reply":"2024-04-21T23:19:49.686306Z"},"trusted":true},"execution_count":87,"outputs":[{"output_type":"display_data","data":{"text/plain":"Parsing NEREL data:   0%|          | 0/65 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"238cb3c4b6e547a981394a40edef98a8"}},"metadata":{}}]},{"cell_type":"markdown","source":"### Write NEREL in spert format","metadata":{}},{"cell_type":"code","source":"if Config.save_nerel:\n    train_spert = sentinfo_to_spert(nerel_to_sentinfo(train_sentences))\n    with open(\"nerel_train.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(train_spert, f)\n        \n    train_train_spert = sentinfo_to_spert(nerel_to_sentinfo(train_train_sentences))\n    with open(\"nerel_train-train.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(train_train_spert, f)\n        \n    train_val_spert = sentinfo_to_spert(nerel_to_sentinfo(train_val_sentences))\n    with open(\"nerel_train-val.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(train_val_spert, f)\n        \n    dev_spert = sentinfo_to_spert_pred(nerel_to_sentinfo(dev_sentences, train=False))\n    with open(\"nerel_dev.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(dev_spert, f)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T18:41:23.652583Z","iopub.execute_input":"2024-04-21T18:41:23.652956Z","iopub.status.idle":"2024-04-21T18:41:29.476599Z","shell.execute_reply.started":"2024-04-21T18:41:23.652928Z","shell.execute_reply":"2024-04-21T18:41:29.475693Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Parsing NEREL data:   0%|          | 0/519 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adf3e455d0c849a79c0bfc80b5fd4f1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Parsing NEREL data:   0%|          | 0/442 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35c6c3e644384ca895dc6d497f62cb1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Parsing NEREL data:   0%|          | 0/77 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0fd55db4b7c4cfcbd9e489c920fc197"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Parsing NEREL data:   0%|          | 0/65 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fa69c648b33486bbb59dee317706be5"}},"metadata":{}}]},{"cell_type":"markdown","source":"### Sanity check","metadata":{}},{"cell_type":"code","source":"# test if batches are tokenized into sentences and detokenized correctly\ndef test_sent_tokenization(nerel_data: List[List[dict]]):\n    print('Sentence tokenization test:')\n    for test_ix, test_data in enumerate(nerel_data, start=1):\n        for ix in tqdm(range(len(test_data)), desc=f'Test {test_ix}/{len(nerel_data)}'):\n            sentence_tokens = sent_tokenize(test_data[ix]['sentences'], language='russian')\n            for i, index in enumerate(tokens_to_indices(test_data[ix]['sentences'], sentence_tokens)):\n                orig = test_data[ix]['sentences'][index[0]:index[1] + 1]\n                tok = sentence_tokens[i]\n                if orig != tok:\n                    print(orig)\n                    print(tok)\n                    assert False, f\"Sentence tokenization test {test_ix}/{len(nerel_data)} failed.\"\n    \n    print('All sentence tokenization tests passed.')\n    return True","metadata":{"execution":{"iopub.status.busy":"2024-04-21T18:38:41.695155Z","iopub.execute_input":"2024-04-21T18:38:41.695523Z","iopub.status.idle":"2024-04-21T18:38:41.703774Z","shell.execute_reply.started":"2024-04-21T18:38:41.695491Z","shell.execute_reply":"2024-04-21T18:38:41.702648Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def test_word_tokenization(nerel_data: List[List[dict]]):\n    print('Word tokenization test:')\n    for test_ix, test_data in enumerate(nerel_data, start=1):\n        sentinfo = nerel_to_sentinfo(test_data)\n        for ix in tqdm(range(len(sentinfo)), desc=f'Test {test_ix}/{len(nerel_data)}'):\n            sent = sentinfo[ix]['sentence']\n            word_tokens = word_tokenize(sent, language='russian')\n            for token_ix, index in enumerate(tokens_to_indices(sentinfo[ix]['sentence'], word_tokens)):\n                orig = sentinfo[ix]['sentence'][index[0]:index[1] + 1]\n                tok = word_tokens[token_ix]\n                # fix quoting bug\n                if (tok == \"``\" or tok == \"''\") and orig == '\\\"':\n                    tok = '\\\"'\n                if orig != tok:\n                    print(orig)\n                    print(tok)\n                    assert False, f\"Word tokenization test {test_ix}/{len(nerel_data)} failed.\"\n    \n    print('All word tokenization tests passed.')\n    return True","metadata":{"execution":{"iopub.status.busy":"2024-04-21T18:38:41.705488Z","iopub.execute_input":"2024-04-21T18:38:41.705816Z","iopub.status.idle":"2024-04-21T18:38:41.724935Z","shell.execute_reply.started":"2024-04-21T18:38:41.705793Z","shell.execute_reply":"2024-04-21T18:38:41.723824Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"tests = [\n    test_sent_tokenization([train_sentences, dev_sentences]),\n    test_word_tokenization([train_sentences, dev_sentences])\n]\n\nif all(tests):\n    print('All tests passed.')\nelse:\n    failed_count = len(list(filter(lambda x: not x, tests)))\n    print(f'{failed_count}/{len(tests)} tests failed.')","metadata":{"execution":{"iopub.status.busy":"2024-04-21T18:38:41.726268Z","iopub.execute_input":"2024-04-21T18:38:41.726586Z","iopub.status.idle":"2024-04-21T18:38:46.640447Z","shell.execute_reply.started":"2024-04-21T18:38:41.726561Z","shell.execute_reply":"2024-04-21T18:38:46.639207Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Sentence tokenization test:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Test 1/2:   0%|          | 0/519 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a90bc5154d74192908391cefb75cf21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Test 2/2:   0%|          | 0/65 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0334b3d9ae1411ab766d755f3023e69"}},"metadata":{}},{"name":"stdout","text":"All sentence tokenization tests passed.\nWord tokenization test:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Parsing NEREL data:   0%|          | 0/519 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73ec3096531140bda3ea0b84ad136cd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Test 1/2:   0%|          | 0/6278 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6ec95b0aba646c784557e76566c75f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Parsing NEREL data:   0%|          | 0/65 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57736f68c4914b90b0d07ace59cc5364"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m tests \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     test_sent_tokenization([train_sentences, dev_sentences]),\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtest_word_tokenization\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_sentences\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m ]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(tests):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAll tests passed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","Cell \u001b[0;32mIn[12], line 4\u001b[0m, in \u001b[0;36mtest_word_tokenization\u001b[0;34m(nerel_data)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord tokenization test:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test_ix, test_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(nerel_data, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     sentinfo \u001b[38;5;241m=\u001b[39m \u001b[43mnerel_to_sentinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ix \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sentinfo)), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_ix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(nerel_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      6\u001b[0m         sent \u001b[38;5;241m=\u001b[39m sentinfo[ix][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m]\n","Cell \u001b[0;32mIn[7], line 19\u001b[0m, in \u001b[0;36mnerel_to_sentinfo\u001b[0;34m(nerel_data, train)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train:\n\u001b[1;32m     18\u001b[0m     ners \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ner \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnerel_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mners\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;66;03m# compute offset relative to sentence\u001b[39;00m\n\u001b[1;32m     21\u001b[0m         ner_offset \u001b[38;5;241m=\u001b[39m ner[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m sent_offset[\u001b[38;5;241m0\u001b[39m], ner[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m sent_offset[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     22\u001b[0m         sent_len \u001b[38;5;241m=\u001b[39m sent_offset[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m sent_offset[\u001b[38;5;241m0\u001b[39m]\n","\u001b[0;31mKeyError\u001b[0m: 'ners'"],"ename":"KeyError","evalue":"'ners'","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}